{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Explore the Preprocessed Dataset\n",
    "In this step, we will:\n",
    "1. Load the preprocessed dataset (`preprocessed_dataset.csv`) created in Notebook 1.\n",
    "2. Explore the structure of the dataset to ensure it is ready for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>task_description</th>\n",
       "      <th>captions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nVbIUDjzWY4</td>\n",
       "      <td>Cars &amp; Other Vehicles</td>\n",
       "      <td>Motorcycles</td>\n",
       "      <td>Paint a Motorcycle</td>\n",
       "      <td>{'start': [13.64, 15.86, 20.6, 23.96, 26.36, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rwmt7Cbuvfs</td>\n",
       "      <td>Cars &amp; Other Vehicles</td>\n",
       "      <td>Motorcycles</td>\n",
       "      <td>Paint a Motorcycle</td>\n",
       "      <td>{'start': [1.8, 6.32, 7.32, 10.86, 13.28, 15.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HnTLh99gcxY</td>\n",
       "      <td>Cars &amp; Other Vehicles</td>\n",
       "      <td>Motorcycles</td>\n",
       "      <td>Paint a Motorcycle</td>\n",
       "      <td>{'start': [0.03, 2.37, 4.29, 6.69, 8.42, 8.67,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RAidUDTPZ-k</td>\n",
       "      <td>Cars &amp; Other Vehicles</td>\n",
       "      <td>Motorcycles</td>\n",
       "      <td>Paint a Motorcycle</td>\n",
       "      <td>{'start': [0.06, 1.38, 3.03, 5.13, 7.44, 8.73,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tYQoPHwNkho</td>\n",
       "      <td>Cars &amp; Other Vehicles</td>\n",
       "      <td>Motorcycles</td>\n",
       "      <td>Paint a Motorcycle</td>\n",
       "      <td>{'start': [0.0, 6.93, 8.94, 11.07, 12.71, 15.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id             category_1   category_2    task_description  \\\n",
       "0  nVbIUDjzWY4  Cars & Other Vehicles  Motorcycles  Paint a Motorcycle   \n",
       "1  rwmt7Cbuvfs  Cars & Other Vehicles  Motorcycles  Paint a Motorcycle   \n",
       "2  HnTLh99gcxY  Cars & Other Vehicles  Motorcycles  Paint a Motorcycle   \n",
       "3  RAidUDTPZ-k  Cars & Other Vehicles  Motorcycles  Paint a Motorcycle   \n",
       "4  tYQoPHwNkho  Cars & Other Vehicles  Motorcycles  Paint a Motorcycle   \n",
       "\n",
       "                                            captions  \n",
       "0  {'start': [13.64, 15.86, 20.6, 23.96, 26.36, 2...  \n",
       "1  {'start': [1.8, 6.32, 7.32, 10.86, 13.28, 15.6...  \n",
       "2  {'start': [0.03, 2.37, 4.29, 6.69, 8.42, 8.67,...  \n",
       "3  {'start': [0.06, 1.38, 3.03, 5.13, 7.44, 8.73,...  \n",
       "4  {'start': [0.0, 6.93, 8.94, 11.07, 12.71, 15.2...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "preprocessed_data = pd.read_csv(\"preprocessed_dataset.csv\")\n",
    "\n",
    "# Display the first few rows to verify the structure\n",
    "preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Text Data for Fine-Tuning\n",
    "In this step, we will:\n",
    "1. Extract the `captions` as input data (source text) and `task_description` as target summaries.\n",
    "2. Process the text data to ensure it is formatted correctly for the fine-tuning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source (Captions): {'start': [13.64, 15.86, 20.6, 23.96, 26.36, 29.36, 32.0, 35.33, 37.69, 40.67, 42.65, 44.57, 47.08, 48.89, 51.19, 53.96, 57.22, 59.23, 60.58, 62.78, 64.15, 66.35, 68.21000000000001, 69.83, 71.81, 73.82, 76.28, 77.78, 79.64, 81.2, 82.97, 86.06, 87.95, 89.3, 91.58, 93.11, 96.95, 99.41, 102.4, 104.65, 106.73, 110.63, 112.58, 114.83, 118.31, 120.47, 123.8, 125.54, 127.7, 130.0, 131.84, 132.62, 134.45, 136.06, 138.53, 140.29, 143.93, 145.97, 147.12, 149.48, 153.48, 155.67000000000004, 159.59, 161.57999999999996, 165.15, 165.66, 167.91, 170.16, 171.62, 174.98, 177.93, 179.73, 181.11, 183.54, 186.56, 188.43, 189.93, 191.26, 193.98, 195.75, 198.29, 202.29, 204.42, 205.68, 206.22, 208.59, 211.47, 213.03, 215.63, 216.93, 218.04, 219.56, 222.54, 225.9, 227.66, 230.48, 232.16, 233.73, 236.43, 238.79, 242.43, 243.93, 246.32, 248.25, 250.07, 252.35, 254.43, 257.45, 259.56, 262.65, 264.63, 266.37, 270.38, 272.52, 274.47, 279.68, 282.71, 286.49, 289.61, 294.08, 296.81, 298.67, 301.61, 303.89, 306.11, 311.36, 315.74, 320.45, 324.08, 326.75, 332.12, 336.65, 338.87, 340.28, 343.31, 345.62, 347.12, 348.95, 352.52, 355.13, 357.32, 359.78, 361.28, 363.74, 367.25, 369.68, 373.43, 375.44, 377.93, 380.66, 384.23, 386.81, 388.86, 391.58, 393.17, 396.17, 398.27, 400.93, 402.83, 406.01, 407.81, 408.16, 409.96, 411.71, 413.81, 415.06, 416.06, 417.31, 419.18, 421.66, 424.18, 426.31], 'end': [18.05, 20.6, 26.36, 29.36, 32.0, 35.33, 37.69, 40.67, 42.65, 44.57, 47.08, 48.89, 51.19, 53.96, 57.22, 59.23, 60.58, 62.78, 64.15, 66.35, 68.21000000000001, 69.83, 71.81, 73.82, 76.28, 77.78, 79.64, 81.2, 82.97, 86.06, 87.95, 89.3, 91.58, 93.11, 96.95, 99.41, 102.4, 104.65, 106.73, 108.95, 110.63, 114.83, 118.31, 120.47, 123.8, 125.54, 127.7, 130.0, 131.84, 132.62, 134.45, 136.06, 138.53, 140.29, 143.93, 145.97, 147.12, 149.48, 151.11, 153.48, 159.59, 161.57999999999996, 165.15, 165.66, 167.91, 170.16, 171.62, 174.98, 177.93, 179.73, 181.11, 183.54, 186.56, 188.43, 189.93, 191.26, 193.98, 195.75, 198.29, 202.29, 204.42, 205.68, 206.22, 208.59, 211.47, 213.03, 215.63, 216.93, 218.04, 219.56, 222.54, 225.9, 227.66, 230.48, 232.16, 233.73, 236.43, 238.79, 242.43, 243.93, 246.32, 248.25, 250.07, 252.35, 254.43, 257.45, 259.56, 262.65, 264.63, 266.37, 270.38, 272.52, 274.47, 276.45, 279.68, 286.49, 289.61, 294.08, 296.81, 298.67, 301.61, 303.89, 306.11, 311.36, 315.74, 320.45, 324.08, 326.75, 332.12, 336.65, 338.87, 340.28, 343.31, 345.62, 347.12, 348.95, 352.52, 355.13, 357.32, 359.78, 361.28, 363.74, 367.25, 369.68, 373.43, 375.44, 377.93, 380.66, 384.23, 386.81, 388.86, 391.58, 393.17, 396.17, 398.27, 400.93, 402.83, 406.01, 407.81, 408.16, 409.96, 411.71, 413.81, 415.06, 416.06, 417.31, 419.18, 421.66, 424.18, 426.31, 429.15, 429.15], 'text': ['folks roylott v twins', 'aids pink project', 'primed pieces guide coded', 'block times got', 'nice surfaces paint job', 'walmart preparing', 'get ready exterior paint job', 'inside tunnel tank', 'inside fender', 'care rear', 'fenders line', 'camera', 'primed anti', 'corrosive primer took red scuff pad', 'scuff pad inside', 'area', 'welded brackets fender', 'little seam sealer', 'pieces', 'tape paper', 'outside flat', 'block inside', 'paint job black', 'clean', 'get into spending', 'lot time putting base coat clear', 'coat inside', \"fender nobody's\", 'sense', 'tape', 'trim block got nice', 'semi gloss blacked', 'inside', 'inside tunnel', 'tape get', 'stuff primed', 'setup get things get garage prep', 'get jigs', 'get defenders', 'tank mounted get ready paint', 'point come', 'black base', 'coat folks', 'parts sprayed', 'inside pieces trim', 'black gives satin black', 'finish nice quick', 'cleans paint', 'gloss black outside', 'visible', 'under', 'acceptable grab camera', \"what's\", 'connell', 'inside fender', 'spread seam', 'sealer welding', 'cleans air', 'fender', 'blacked inside', 'dry pretty good prep', 'garage get get ready', 'items paint', 'fixtures stands get', 'painted', 'set come', 'go paint process', 'get base coat', 'got', 'garage blown', 'wet floor edges', 'walls nooks crannies', 'hose seal dust', 'took bench covered', 'masking paper fixtures', 'fenders couple', 'masking paper', 'taken fenders got', 'wall mounted', 'minute precaution', 'took red scuff pad went quickly', 'fenders thing', 'compressed air', 'mate', 'air hose blow', 'fixture fenders little', 'nooks crannies make', 'blow dust', 'little water', \"what's\", 'anthon things nature', 'get squared away', 'wash pretty cleaner', 'point pretty ready', 'go get gun set pink ball', 'set base', 'tape folks get blowing', 'washed pre cleanup', 'looked good good', 'go got garage set', 'closed lights', 'turn exhaust fan got gun', 'set got face coat', 'little close', 'got', \"here's tank got set\", 'jig past', 'works good', '2x4 attach', 'screws end sit', 'end', 'tank', 'flip get', 'good', 'flip', 'seesaws forth got', 'set got fender', 'jig got', 'fender jig got', 'spray gun got base coat', 'mixed base coat uh', 'sherwin williams dimension mixin', 'mixing ratio parts base', 'reducer', 'medium coats', 'surface tap', 'pieces get fan gun', 'get paint strained into gun', 'coat base', 'idea cope', 'cover piece color', 'coat got', 'able', 'panic', 'flash depending', 'environment mean worried', 'seventy eighty degrees', 'pretty perfect flash', 'minutes', 'notice little matte flat', 'finish get', 'eye finish cup', 'finished', 'application wait 10 15 minutes', 'coat coats', 'covered', 'think coat', 'recommend', 'flash go', 'edges sides undersides', 'make areas', 'paint', 'clear', 'start process', 'minutes', 'check areas covered', 'gun dust areas', 'make big difference', 'happened painter', 'paint got', 'little', 'said', \"didn't hit\", 'certain angle', 'sitting paint', 'screwed', 'gotta sand thing', 'paint get caught', 'make check edges', 'apply base coat', 'come talk', 'clear coat']}\n",
      "Target (Task Description): Paint a Motorcycle\n",
      "--------------------------------------------------\n",
      "Source (Captions): {'start': [1.8, 6.32, 7.32, 10.86, 13.28, 15.62, 20.42, 24.92, 26.74, 29.64, 33.98, 34.98, 38.93, 40.01, 43.75, 48.1, 51.0, 55.66, 59.32, 61.35, 64.7, 67.83, 71.34, 75.5, 77.87, 82.48, 86.9, 88.53, 93.28, 98.06, 101.56, 105.5, 107.07, 110.92, 113.23, 117.49, 119.95, 124.39, 127.39, 132.42, 137.04, 140.23, 143.49, 144.49, 148.09, 152.82, 158.3], 'end': [3.42, 6.92, 10.58, 12.66, 15.2, 19.58, 24.68, 26.74, 29.64, 33.98, 34.98, 38.93, 40.01, 43.75, 48.1, 51.0, 55.48, 59.2, 61.34, 64.69, 67.83, 71.34, 75.5, 77.86, 82.48, 86.9, 88.53, 93.28, 98.06, 101.56, 105.5, 107.07, 110.92, 113.23, 117.49, 119.95, 124.39, 127.39, 132.42, 137.04, 139.23, 143.49, 144.49, 146.38, 152.8, 156.06, 160.76], 'text': ['plasti dip rims', 'begin', 'thing clean dog piss rims', 'literally metaphorically', 'dry rim thoroughly clean rag', 'tape bearings transparent tape block threads mount brake disks', 'able stuff random paper amp plastic laying garage cause', 'professional', 'prepped sides time get spraying', 'make lit ventilated area rim contact', 'sun', 'thing asshole star 93 million miles away fuck', 'paint job', 'shaking vigorously full minute told bike', 'slow test spray piece cardboard paper', 'cat test piece choice', 'coat start light coat 50 60 coverage', 'spray painting thin layer help create bond', 'disperse full coat', 'spray short bursts spraying', 'coat dry 30 min', 'spray second coat 95 100 coverage', 'main goal apply coat evenly entire wheel', 'allow 30 min dry', 'based particular plasti dip project fourth coat', \"could've sprayed fourth layer decided coat\", 'thicker layer', 'finally achieve attractive shine match bald head provide protection', 'form elements added 1 coat plasti dip enhancer glossifier', 'hour remove excess plasti dip tire', 'contrary suggest using painters tape chicken strips', 'preparation', 'choose use brake cleaner remove plasti dip', 'thin peel', 'finally time pat admire work amp cure', '8 hours install tire', 'alright think came looking pretty fan fuckin tastic', 'got value', 'spread love ask questions respond', 'wanna pictures post pictures instagram', 'tomorrow get bike', 'watching catch later', 'peace', 'alright', 'alright hahaha enjoy', 'comment subscribe god damnit', 'hahahaha shit']}\n",
      "Target (Task Description): Paint a Motorcycle\n",
      "--------------------------------------------------\n",
      "Source (Captions): {'start': [0.03, 2.37, 4.29, 6.69, 8.42, 8.67, 12.69, 14.16, 16.13, 18.0, 20.27, 22.65, 25.85, 28.05, 30.75, 32.96, 35.61, 36.78, 39.11, 41.07, 42.98, 67.61, 70.56, 73.28, 74.53999999999998, 76.28, 79.49, 81.05, 82.85, 84.86, 121.58, 124.93, 150.7, 153.98, 155.64, 157.37, 160.25, 163.04, 167.48, 168.84, 171.06, 191.51, 193.98, 196.48, 198.73, 201.09, 203.98, 207.9, 210.09, 215.2, 219.96, 224.81, 229.02, 232.37, 241.97, 245.0, 278.83, 281.93, 284.62, 286.21, 287.96, 289.55, 299.75, 302.25, 306.57, 309.12, 311.28, 313.35, 315.6, 323.47, 326.72, 329.21, 331.28, 332.99, 335.68, 336.97, 340.12, 342.3400000000001, 344.12, 346.4, 348.65, 351.13, 353.38, 356.27], 'end': [4.29, 6.69, 8.42, 8.67, 12.69, 14.16, 16.13, 18.0, 20.27, 22.65, 25.85, 28.05, 30.75, 32.96, 35.61, 36.78, 39.11, 41.07, 42.98, 44.39, 47.71, 73.28, 74.53999999999998, 76.28, 79.18, 79.18, 82.85, 84.86, 87.7, 87.7, 129.27, 129.27, 155.64, 157.37, 160.25, 163.04, 167.48, 168.84, 171.06, 173.45, 173.45, 196.48, 198.73, 201.09, 203.98, 207.9, 210.09, 215.2, 219.96, 224.81, 228.68, 228.68, 237.06, 237.06, 248.89, 248.89, 284.62, 286.21, 287.96, 289.55, 292.0, 292.0, 306.57, 309.12, 311.28, 313.35, 315.6, 319.04, 319.04, 329.21, 331.28, 332.99, 335.68, 336.97, 340.12, 342.3400000000001, 344.12, 346.4, 348.65, 351.13, 353.38, 356.27, 358.41, 358.41], 'text': [\"previously customer mods they've\", 'faded damage paint motorcycle', 'tasted thing', 'primer spray', 'booth', 'pretty good custom paint job', 'decided paint', 'bike track race bearings', 'tank', 'plastic parts paint', 'flurry color suggested', 'picked', 'color scheme n gm ford', 'racing team colors big fan', 'motogp thought color', 'scheme ago uh prepared', 'clean oil wax grease', 'remover spray 2k', 'transparent sealer', 'help new paint stick', 'ready start color scheme', '2k cielo niche promoter', 'lot quality', 'especially', 'asking outlines helps paint', 'stick lot', 'got lot black colors', 'game bring bright black', 'pull pots ready', 'start mopping', 'best time data people', 'file tents', 'master lines', 'color', 'spray white', 'fluo colours', 'go white base', 'get reflective book instead', 'playing black', 'white british', 'bluest', 'using spray cheap', 'fluorescent colors spray cheap', 'australian company', 'australia provide quality', 'custom paints including candies curls', 'custom tanks yes good', 'start', 'masking lines', 'color orange', 'red thread', 'watches', 'letter color scheme', 'hopefully works excuse', 'won lansford', 'carried', 'dude zero colors spreadsheet bass', 'player clearer debate color', 'spray bass', 'dry', 'minutes spray clear', 'coat', 'spray pack', 'satin tweeter 50', 'cent gloss full', 'gloss', '2 pack detection', 'fuel resistance chemical resistance', 'motorcycles', 'go paint bike', 'fluro color scheme real damage', 'paint work start paint', 'stripped got bare', 'metal bare plastic painted', 'thing color scheme wanted', 'using fluro color got', 'white base fluro', 'sweep', 'paint colors', 'make get white', 'fluo clear', 'coat protect pretty', 'ho finish', 'time']}\n",
      "Target (Task Description): Paint a Motorcycle\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Extract source (captions) and target (task_description) text\n",
    "source_text = preprocessed_data[\"captions\"]\n",
    "target_text = preprocessed_data[\"task_description\"]\n",
    "\n",
    "# Display a sample of the source and target text for verification\n",
    "for i in range(3):  # Display the first 3 examples\n",
    "    print(f\"Source (Captions): {source_text[i]}\")\n",
    "    print(f\"Target (Task Description): {target_text[i]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Process Captions into Plain Text\n",
    "The `captions` column does not need additional evaluation with `eval()`. We will:\n",
    "1. Access the `text` key in the dictionaries stored in the `captions` column.\n",
    "2. Concatenate all captions into a single string for each video.\n",
    "3. Handle any missing or malformed captions gracefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Processing of Captions\n",
    "The `captions` column is already structured as dictionaries, so:\n",
    "1. We will directly access the `text` key.\n",
    "2. Concatenate the strings in the `text` list into a single string for each video.\n",
    "3. Store the result in a new column, `processed_captions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Captions: \n",
      "Target (Task Description): Paint a Motorcycle\n",
      "--------------------------------------------------\n",
      "Processed Captions: \n",
      "Target (Task Description): Paint a Motorcycle\n",
      "--------------------------------------------------\n",
      "Processed Captions: \n",
      "Target (Task Description): Paint a Motorcycle\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Simplified function to process captions into plain text\n",
    "def process_captions(caption_data):\n",
    "    try:\n",
    "        # Extract the 'text' field and join all captions into a single string\n",
    "        return \" \".join(caption_data[\"text\"])\n",
    "    except (TypeError, KeyError):\n",
    "        # Handle missing or malformed captions\n",
    "        return \"\"\n",
    "\n",
    "# Apply the function directly to the captions column\n",
    "preprocessed_data[\"processed_captions\"] = preprocessed_data[\"captions\"].apply(process_captions)\n",
    "\n",
    "# Display a sample of the processed captions\n",
    "for i in range(3):  # Display the first 3 examples\n",
    "    print(f\"Processed Captions: {preprocessed_data['processed_captions'][i]}\")\n",
    "    print(f\"Target (Task Description): {target_text[i]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Split Data into Training, Validation, and Testing Sets\n",
    "\n",
    "**What We Are Doing:**\n",
    "\n",
    "**Purpose:** Divide the preprocessed data into three subsets to evaluate the model's performance during and after training.\n",
    "\n",
    "**Steps:**\n",
    "1. Split the data into training + validation (90%) and testing (10%) subsets.\n",
    "2. Further divide the training + validation subset into training (80%) and validation (10%) subsets.\n",
    "3. Shuffle the data to ensure examples are well-distributed.\n",
    "4. Set a random seed for reproducibility of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 1238867\n",
      "Reduced dataset size: 124\n",
      "Training set size: 98\n",
      "Validation set size: 13\n",
      "Testing set size: 13\n",
      "Unique values in category_1:\n",
      "['Food and Entertaining' 'Hobbies and Crafts' 'Pets and Animals'\n",
      " 'Personal Care and Style' 'Home and Garden'\n",
      " 'Education and Communications' 'Cars & Other Vehicles' 'Health'\n",
      " 'Holidays and Traditions' 'Arts and Entertainment' 'Sports and Fitness'\n",
      " 'Computers and Electronics']\n",
      "\n",
      "Unique values in category_2:\n",
      "['Recipes' 'Tricks and Pranks' 'Dogs' 'Drinks' 'Grooming'\n",
      " 'Food Preparation' 'Gardening' 'Housekeeping' 'Speaking'\n",
      " 'Care and Use of Cooking Equipment' 'Driving Techniques' 'Crafts'\n",
      " 'Alternative Health' 'Tools' 'Parties' 'Easter' 'Subjects'\n",
      " 'Landscaping and Outdoor Building' 'Herbs and Spices' 'Music'\n",
      " 'Home Improvements and Repairs' 'Collecting' 'Individual Sports'\n",
      " \"Mother's Day\" 'Cars' 'Crustaceans' 'Toys' 'Motorcycles' 'Vehicle Sports'\n",
      " 'Holiday Cooking' 'Bicycles' 'TV and Home Audio' nan 'Woodworking'\n",
      " 'Emotional Health']\n",
      "\n",
      "Sample training data:\n",
      "            video_id             category_1        category_2  \\\n",
      "966999   q-4IL2MadHQ        Home and Garden      Housekeeping   \n",
      "22832    bKKvoXMtCls     Hobbies and Crafts            Crafts   \n",
      "176916   9lEwYm9mpyE  Food and Entertaining           Recipes   \n",
      "1118917  bRtDAsi6UlY  Food and Entertaining           Recipes   \n",
      "754040   PvkdCJG4Qc0  Food and Entertaining  Food Preparation   \n",
      "\n",
      "                          task_description  \\\n",
      "966999     Clean a Dishwasher with Vinegar   \n",
      "22832                     Make a Rag Quilt   \n",
      "176916               Make Crawfish Etouffe   \n",
      "1118917  Make Holiday Ice Cream Sandwiches   \n",
      "754040                            Can Corn   \n",
      "\n",
      "                                                  captions processed_captions  \n",
      "966999   {'start': [0.03, 1.77, 4.89, 7.44, 9.8, 12.5, ...                     \n",
      "22832    {'start': [2.24, 5.1, 7.83, 12.45, 15.78, 17.7...                     \n",
      "176916   {'start': [4.59, 7.99, 9.7, 11.86, 14.13, 17.2...                     \n",
      "1118917  {'start': [0.03, 2.49, 4.58, 6.45, 8.3, 10.55,...                     \n",
      "754040   {'start': [3.29, 5.91, 7.98, 10.29, 17.03, 20....                     \n",
      "\n",
      "Sample validation data:\n",
      "            video_id             category_1                        category_2  \\\n",
      "32392    4I-Fd11I02k  Food and Entertaining                           Recipes   \n",
      "945657   3I2HwNiYeyU  Food and Entertaining                            Drinks   \n",
      "1131896  8ZtQSZugi5A  Cars & Other Vehicles                    Vehicle Sports   \n",
      "130576   lIMzpl-QrcM     Hobbies and Crafts                        Collecting   \n",
      "785097   KFM3RZNl3Bw        Home and Garden  Landscaping and Outdoor Building   \n",
      "\n",
      "                    task_description  \\\n",
      "32392     Make Sugar Cookie Biscuits   \n",
      "945657   Freeze Your Smoothie Greens   \n",
      "1131896                  Drift a Car   \n",
      "130576        Polish Quartz Crystals   \n",
      "785097                 Seal Concrete   \n",
      "\n",
      "                                                  captions processed_captions  \n",
      "32392    {'start': [15.02, 17.55, 20.22, 23.51, 25.05, ...                     \n",
      "945657   {'start': [0.0, 2.76, 5.45, 8.17, 12.24, 16.37...                     \n",
      "1131896  {'start': [9.39, 12.25, 14.11, 15.97, 18.18, 2...                     \n",
      "130576   {'start': [0.03, 3.0, 8.06, 11.61, 13.38, 16.8...                     \n",
      "785097   {'start': [0.03, 2.25, 5.8100000000000005, 7.5...                     \n",
      "\n",
      "Sample testing data:\n",
      "            video_id              category_1 category_2  \\\n",
      "1018054  4wWSNz9DRDU   Food and Entertaining    Recipes   \n",
      "1211065  bSluSDOICDI  Arts and Entertainment      Music   \n",
      "145119   EOZU6t01fXo      Hobbies and Crafts     Crafts   \n",
      "549945   hgnPUEhBpVM   Food and Entertaining    Recipes   \n",
      "260217   nBSncN7uup0        Pets and Animals       Dogs   \n",
      "\n",
      "                      task_description  \\\n",
      "1018054            Prepare Sooji Halwa   \n",
      "1211065        Change a Ukulele String   \n",
      "145119       Make Lollipop Flower Pots   \n",
      "549945          Make Cauliflower Salad   \n",
      "260217   Make an Emergency Pet Harness   \n",
      "\n",
      "                                                  captions processed_captions  \n",
      "1018054  {'start': [7.73, 11.34, 13.01, 15.45, 17.78, 2...                     \n",
      "1211065  {'start': [0.58, 6.5600000000000005, 9.09, 13....                     \n",
      "145119   {'start': [0.06, 2.73, 4.58, 6.87, 10.29, 12.7...                     \n",
      "549945   {'start': [0.0, 2.31, 6.33, 8.25, 11.96, 13.79...                     \n",
      "260217   {'start': [0.06, 2.42, 4.74, 7.29, 9.33, 11.13...                     \n"
     ]
    }
   ],
   "source": [
    "# Step 4: Split Data into Training, Validation, and Testing Sets with Dataset Reduction\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Display the original dataset size\n",
    "print(f\"Original dataset size: {len(preprocessed_data)}\")\n",
    "\n",
    "# Reduce the dataset to 10% of the original size for faster processing\n",
    "reduced_data = preprocessed_data.sample(frac=0.0001, random_state=42)\n",
    "\n",
    "# Display the reduced dataset size\n",
    "print(f\"Reduced dataset size: {len(reduced_data)}\")\n",
    "\n",
    "# Optional: Inspect the distribution of any categories (if available)\n",
    "if 'category' in reduced_data.columns:  # Adjust column name if needed\n",
    "    print(\"Distribution of categories in reduced dataset:\")\n",
    "    print(reduced_data['category'].value_counts())\n",
    "\n",
    "# Split the reduced dataset into train+validation (90%) and test (10%)\n",
    "train_val_data, test_data = train_test_split(\n",
    "    reduced_data, test_size=0.1, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Further split train+validation into training (80%) and validation (10%)\n",
    "train_data, val_data = train_test_split(\n",
    "    train_val_data, test_size=0.111, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Display the sizes of the datasets\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")\n",
    "print(f\"Testing set size: {len(test_data)}\")\n",
    "\n",
    "# Print unique values in category_1 and category_2 columns\n",
    "print(\"Unique values in category_1:\")\n",
    "print(reduced_data['category_1'].unique())\n",
    "\n",
    "print(\"\\nUnique values in category_2:\")\n",
    "print(reduced_data['category_2'].unique())\n",
    "\n",
    "# Display a few rows from each dataset to verify\n",
    "print(\"\\nSample training data:\")\n",
    "print(train_data.head())\n",
    "\n",
    "print(\"\\nSample validation data:\")\n",
    "print(val_data.head())\n",
    "\n",
    "print(\"\\nSample testing data:\")\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Download Videos from YouTube\n",
    "\n",
    "This step will:\n",
    "\n",
    "1. Use the video IDs in the training, validation, and testing datasets.\n",
    "2. Download videos using **yt-dlp**.\n",
    "3. Save the videos in a structured directory (e.g., `videos/train/`, `videos/val/`, `videos/test/`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading training videos...\n",
      ".................................x.x....x......x...x.x.....xx............x.......x.....x..........\n",
      "Downloading validation videos...\n",
      "......x.xx..x\n",
      "Downloading testing videos...\n",
      ".x.......x...\n",
      "\n",
      "Video downloads complete.\n"
     ]
    }
   ],
   "source": [
    "from yt_dlp import YoutubeDL\n",
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Create a logger to suppress yt-dlp logs\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "# Define the output directories for videos\n",
    "video_dirs = {\n",
    "    \"train\": \"videos/train/\",\n",
    "    \"val\": \"videos/val/\",\n",
    "    \"test\": \"videos/test/\"\n",
    "}\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for dir_path in video_dirs.values():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Function to download a video using yt-dlp\n",
    "def download_video(video_id, output_dir):\n",
    "    url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "    ydl_opts = {\n",
    "        \"format\": \"best\",  # Best available quality\n",
    "        \"outtmpl\": f\"{output_dir}/{video_id}.mp4\",  # Output filename\n",
    "        \"quiet\": True,  # Suppress yt-dlp logs\n",
    "        \"no_warnings\": True,  # Suppress yt-dlp warnings\n",
    "        \"logger\": logging.getLogger()  # Suppress yt-dlp messages\n",
    "    }\n",
    "    with YoutubeDL(ydl_opts) as ydl:\n",
    "        try:\n",
    "            ydl.download([url])\n",
    "            print(\".\", end=\"\", flush=True)  # Print a dot for success\n",
    "        except Exception:\n",
    "            print(\"x\", end=\"\", flush=True)  # Print an 'x' for failure\n",
    "\n",
    "# Download videos for each dataset\n",
    "print(\"Downloading training videos...\")\n",
    "for video_id in train_data[\"video_id\"]:\n",
    "    download_video(video_id, video_dirs[\"train\"])\n",
    "\n",
    "print(\"\\nDownloading validation videos...\")\n",
    "for video_id in val_data[\"video_id\"]:\n",
    "    download_video(video_id, video_dirs[\"val\"])\n",
    "\n",
    "print(\"\\nDownloading testing videos...\")\n",
    "for video_id in test_data[\"video_id\"]:\n",
    "    download_video(video_id, video_dirs[\"test\"])\n",
    "\n",
    "print(\"\\n\\nVideo downloads complete.\")  # Print a new line after downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Preprocess Videos for Feature Extraction\n",
    "\n",
    "This step will:\n",
    "\n",
    "1. Extract frames or embeddings from the videos.\n",
    "2. Save the extracted features in structured directories for training, validation, and testing datasets.\n",
    "3. Use efficient processing libraries such as OpenCV, PyTorch, or pre-trained models like CLIP for feature extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [05:18<00:00,  3.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing val videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:36<00:00,  4.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:51<00:00,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Define directories for the videos and features\n",
    "video_dirs = {\n",
    "    \"train\": \"videos/train/\",\n",
    "    \"val\": \"videos/val/\",\n",
    "    \"test\": \"videos/test/\",\n",
    "}\n",
    "\n",
    "feature_dirs = {\n",
    "    \"train\": \"features/train/\",\n",
    "    \"val\": \"features/val/\",\n",
    "    \"test\": \"features/test/\",\n",
    "}\n",
    "\n",
    "# Create feature directories if they don't exist\n",
    "for dir_path in feature_dirs.values():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Pre-trained model for visual feature extraction (e.g., ResNet)\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = resnet50(pretrained=True)\n",
    "model = model.eval().to(device)\n",
    "\n",
    "# Transform to preprocess video frames\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def extract_frames(video_path, frame_rate=1):\n",
    "    \"\"\"\n",
    "    Extract frames from a video file at the given frame rate.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_interval = fps // frame_rate\n",
    "\n",
    "    while cap.isOpened():\n",
    "        frame_id = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_id % frame_interval == 0:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "            frames.append(Image.fromarray(frame))\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def extract_features(frames, model, transform, device):\n",
    "    \"\"\"\n",
    "    Extract features for a list of frames using a pre-trained model.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for frame in frames:\n",
    "        frame_tensor = transform(frame).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            feature = model(frame_tensor).cpu().numpy()\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "# Process videos for each dataset\n",
    "for split, video_dir in video_dirs.items():\n",
    "    print(f\"Processing {split} videos...\")\n",
    "    for video_file in tqdm(os.listdir(video_dir)):\n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        if not video_file.endswith(\".mp4\"):\n",
    "            continue\n",
    "\n",
    "        # Extract frames and features\n",
    "        frames = extract_frames(video_path)\n",
    "        features = extract_features(frames, model, transform, device)\n",
    "\n",
    "        # Save features as a numpy file\n",
    "        feature_file = os.path.join(feature_dirs[split], f\"{os.path.splitext(video_file)[0]}.npy\")\n",
    "        np.save(feature_file, features)\n",
    "\n",
    "print(\"Feature extraction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Extracted Features\n",
    "This step will:\n",
    "\n",
    "1. Load a few `.npy` files from the structured directories (e.g., `features/train/`, `features/val/`, `features/test/`).\n",
    "2. Visualize or print out the contents and structure of the feature arrays to ensure that the extraction process worked as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting features in features/train/...\n",
      "\n",
      "File: S0rZQ8d-z_c.npy\n",
      "Shape: (97, 1, 1000)\n",
      "Sample Data: [[[-0.52196467  1.6278666   0.21541211 ... -2.037949    2.278176\n",
      "    3.215495  ]]\n",
      "\n",
      " [[-0.5318004   3.0022192   1.0437046  ... -2.660032    1.0309241\n",
      "    3.0269563 ]]\n",
      "\n",
      " [[-0.35514843  3.2112522  -1.7236078  ... -2.6421385  -0.50810766\n",
      "    0.46100932]]\n",
      "\n",
      " [[-2.37383     2.0584176  -0.9910927  ... -2.2873552   0.19965419\n",
      "    1.1317472 ]]\n",
      "\n",
      " [[-2.1626418   1.8039418  -0.5193502  ... -1.9179364   0.19929169\n",
      "    0.6916184 ]]]\n",
      "--------------------------------------------------\n",
      "File: WEA1XkYkPO4.npy\n",
      "Shape: (817, 1, 1000)\n",
      "Sample Data: [[[-5.8233482e-01  6.8116599e-01 -2.9223970e-01 ... -1.6159886e+00\n",
      "   -1.6310718e-05  1.5598352e+00]]\n",
      "\n",
      " [[-7.9267472e-03  1.0051288e-01 -1.4209548e+00 ... -2.5470219e+00\n",
      "    2.2099760e+00  3.0678093e+00]]\n",
      "\n",
      " [[ 1.2105324e+00  9.4390994e-01 -1.0709611e+00 ... -2.0749018e+00\n",
      "    3.6800654e+00  3.3206577e+00]]\n",
      "\n",
      " [[ 5.5020666e-01  6.6695499e-01 -1.4132812e+00 ... -2.6674950e+00\n",
      "    3.3455524e+00  3.8765180e+00]]\n",
      "\n",
      " [[ 1.5539888e+00  1.2974380e+00 -7.7653205e-01 ... -2.0873806e+00\n",
      "    3.9225330e+00  3.6097114e+00]]]\n",
      "--------------------------------------------------\n",
      "File: IwQ47d1n6cY.npy\n",
      "Shape: (1011, 1, 1000)\n",
      "Sample Data: [[[-2.0891774  -0.31278482 -2.0275593  ... -1.3356626   0.27164114\n",
      "    0.8384365 ]]\n",
      "\n",
      " [[-2.0935874  -0.32572827 -2.0290017  ... -1.3401072   0.26531684\n",
      "    0.8209959 ]]\n",
      "\n",
      " [[-2.0926244  -0.32642254 -2.0293972  ... -1.339294    0.26591492\n",
      "    0.8211373 ]]\n",
      "\n",
      " [[-2.0926244  -0.32642254 -2.0293972  ... -1.339294    0.26591492\n",
      "    0.8211373 ]]\n",
      "\n",
      " [[-2.093232   -0.326033   -2.0292027  ... -1.3397026   0.2649492\n",
      "    0.8206873 ]]]\n",
      "--------------------------------------------------\n",
      "Inspecting features in features/val/...\n",
      "\n",
      "File: KFM3RZNl3Bw.npy\n",
      "Shape: (238, 1, 1000)\n",
      "Sample Data: [[[-1.0486431  -0.31640348  0.8507834  ... -2.3296077  -0.2263678\n",
      "   -1.4279866 ]]\n",
      "\n",
      " [[-0.10990973 -0.4520348   1.1430025  ... -1.9571105   0.3276133\n",
      "   -2.6291287 ]]\n",
      "\n",
      " [[ 0.05378245 -0.03481759  2.0177855  ... -1.5005513   0.14392702\n",
      "   -2.5391212 ]]\n",
      "\n",
      " [[ 0.49760616 -0.14749108  1.9236776  ... -0.49641144 -0.11683839\n",
      "   -2.507205  ]]\n",
      "\n",
      " [[-0.02571763 -0.21168552  1.1167669  ... -1.4304582  -0.14485331\n",
      "   -2.4906356 ]]]\n",
      "--------------------------------------------------\n",
      "File: 8ZtQSZugi5A.npy\n",
      "Shape: (361, 1, 1000)\n",
      "Sample Data: [[[-5.8233482e-01  6.8116599e-01 -2.9223970e-01 ... -1.6159886e+00\n",
      "   -1.6310718e-05  1.5598352e+00]]\n",
      "\n",
      " [[-8.7540567e-01  8.4279668e-01  3.9202189e-01 ... -3.2471700e+00\n",
      "   -1.3791301e+00 -8.0436788e-02]]\n",
      "\n",
      " [[-2.1261184e+00  7.9332626e-01 -6.2462986e-01 ... -3.6778374e+00\n",
      "   -1.6351993e+00 -2.8350180e-01]]\n",
      "\n",
      " [[ 1.1526947e+00  2.2037652e+00  1.6990895e+00 ...  1.7776807e-01\n",
      "    1.0489821e+00 -1.6333139e+00]]\n",
      "\n",
      " [[-8.2906485e-01  2.6246781e+00  1.0028160e+00 ... -1.9942540e+00\n",
      "   -2.4954538e-01 -6.8328696e-01]]]\n",
      "--------------------------------------------------\n",
      "File: VG2MZV7OmCw.npy\n",
      "Shape: (160, 1, 1000)\n",
      "Sample Data: [[[-2.423415    0.13243236 -2.2528052  ...  3.0400708   4.3799634\n",
      "    0.35284048]]\n",
      "\n",
      " [[-2.1223602   0.921832   -1.5310414  ...  4.502453    4.3260007\n",
      "   -0.20160562]]\n",
      "\n",
      " [[-2.1920083   0.31925157 -0.95946956 ...  5.3314886   4.117538\n",
      "    0.37040222]]\n",
      "\n",
      " [[-1.9420412   0.66608036 -1.2941091  ...  4.2177405   4.1439633\n",
      "    1.6054282 ]]\n",
      "\n",
      " [[-2.2535102   0.6893959  -1.6189505  ...  3.9513404   3.9523594\n",
      "    1.2823733 ]]]\n",
      "--------------------------------------------------\n",
      "Inspecting features in features/test/...\n",
      "\n",
      "File: GY8hJdqwUlA.npy\n",
      "Shape: (1252, 1, 1000)\n",
      "Sample Data: [[[ 1.0395772  -0.31217313 -2.6716325  ...  1.1139821   1.0770162\n",
      "    0.5484184 ]]\n",
      "\n",
      " [[ 0.8373869  -0.15956993 -2.6538663  ...  1.120228    1.0362494\n",
      "    0.55542934]]\n",
      "\n",
      " [[ 0.8454112  -0.11309804 -2.8512716  ...  0.82084966  0.6742498\n",
      "    0.90300906]]\n",
      "\n",
      " [[ 0.89631414  0.04904717 -2.925619   ...  0.6962105   0.8303374\n",
      "    0.7653775 ]]\n",
      "\n",
      " [[ 1.4856052   1.2326796  -1.033948   ...  1.5731913   1.631678\n",
      "    0.02327753]]]\n",
      "--------------------------------------------------\n",
      "File: nBSncN7uup0.npy\n",
      "Shape: (165, 1, 1000)\n",
      "Sample Data: [[[-6.6894281e-01  8.0755925e-01 -2.3772481e-01 ... -1.5765547e+00\n",
      "   -2.5833934e-03  1.6915801e+00]]\n",
      "\n",
      " [[ 9.8896861e-01 -1.7790134e+00 -1.8755274e+00 ... -8.4135967e-01\n",
      "   -5.1957381e-01 -2.2887501e-01]]\n",
      "\n",
      " [[ 9.2740250e-01  5.0677651e-01 -1.3389094e+00 ... -1.9820620e+00\n",
      "   -2.1734582e-01 -5.1516944e-01]]\n",
      "\n",
      " [[ 2.2624676e+00 -2.8937087e+00 -2.8696809e+00 ... -2.2579348e+00\n",
      "   -4.1974641e-02 -4.4675803e-01]]\n",
      "\n",
      " [[ 1.1372592e+00 -1.5533670e+00 -1.8868796e+00 ... -2.2791977e+00\n",
      "   -7.2134531e-01  9.7385691e-03]]]\n",
      "--------------------------------------------------\n",
      "File: q5KfolVKQ5Y.npy\n",
      "Shape: (66, 1, 1000)\n",
      "Sample Data: [[[ 2.3159735   1.9684876   0.6644624  ...  1.3042494   0.4124764\n",
      "   -2.7553084 ]]\n",
      "\n",
      " [[ 2.4195423   2.0040119   0.65506434 ...  1.4015832   0.48060846\n",
      "   -2.743754  ]]\n",
      "\n",
      " [[ 1.8161565   0.18516059  0.58627754 ...  2.6886852   1.4902216\n",
      "   -1.7428207 ]]\n",
      "\n",
      " [[ 0.19537292  0.0726793   0.998868   ...  2.405683    1.7845988\n",
      "   -1.7371297 ]]\n",
      "\n",
      " [[ 0.20096557  0.07968211  1.014086   ...  2.374775    1.7482716\n",
      "   -1.7673081 ]]]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the paths to the feature directories\n",
    "feature_dirs = {\n",
    "    \"train\": \"features/train/\",\n",
    "    \"val\": \"features/val/\",\n",
    "    \"test\": \"features/test/\"\n",
    "}\n",
    "\n",
    "# Function to visualize a few `.npy` files\n",
    "def inspect_features(feature_dir, num_samples=3):\n",
    "    print(f\"Inspecting features in {feature_dir}...\\n\")\n",
    "    files = os.listdir(feature_dir)\n",
    "    npy_files = [file for file in files if file.endswith(\".npy\")]\n",
    "    \n",
    "    # Display information for a few `.npy` files\n",
    "    for i, npy_file in enumerate(npy_files[:num_samples]):\n",
    "        feature_path = os.path.join(feature_dir, npy_file)\n",
    "        features = np.load(feature_path)\n",
    "        print(f\"File: {npy_file}\")\n",
    "        print(f\"Shape: {features.shape}\")\n",
    "        print(f\"Sample Data: {features[:5]}\")  # Print first 5 elements\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Inspect features in each dataset\n",
    "for dataset, feature_dir in feature_dirs.items():\n",
    "    if os.path.exists(feature_dir):\n",
    "        inspect_features(feature_dir)\n",
    "    else:\n",
    "        print(f\"Feature directory '{feature_dir}' does not exist.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Prepare Feature Data for Model Training\n",
    "\n",
    "This step will:\n",
    "\n",
    "1. Load the extracted features from the `features/train/`, `features/val/`, and `features/test/` directories.\n",
    "2. Pair the features with their corresponding labels from the training, validation, and testing datasets.\n",
    "3. Prepare the data for input into a machine learning or deep learning model by:\n",
    "   - Creating feature-label pairs.\n",
    "   - Organizing the data into a format compatible with training frameworks like PyTorch or TensorFlow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Unique Labels Encoded: ['Adjust to Driving a Car on the Right Side of the Road'\n",
      " 'Ask a Guy Out over Text' 'Bake a Ring Into a Cake or Other Food'\n",
      " 'Braze Aluminum' 'Bread Chicken' 'Build Shoe Insoles' 'Can Corn'\n",
      " 'Care for Lawn Tools' 'Change a Ukulele String'\n",
      " 'Change the Batteries in a Buzz Lightyear Action Figure' 'Clean Gold'\n",
      " 'Clean a Dishwasher with Vinegar' 'Clean a Man Cave' 'Cook Egg Whites'\n",
      " 'Cook Honey Glazed Parsnips' 'Cook a Chuck Roast'\n",
      " 'Create Window Valances from Cardboard Boxes'\n",
      " 'Create a Hermit Crab Habitat' 'Declutter an Entryway' 'Dig Post Holes'\n",
      " 'Drift a Car' \"Dye Men's Hair\" 'Filter Fry Oil for Reuse'\n",
      " 'Fix Sticky Drawers' 'Fix the Crotch Hole in Your Jeans'\n",
      " 'Freeze Tomatillos' 'Freeze Your Smoothie Greens' 'Fringe a Shirt'\n",
      " 'Get Children Involved With Science' 'Get Rid of Moles in Your Garden'\n",
      " 'Get Rid of a Beehive' 'Grill Turkey' 'Groom a Longhair Dachshund'\n",
      " 'Grow Gerbera Daisies' 'Grow Herbs in Pots' 'Grow Kohlrabi'\n",
      " 'Grow Redwoods from Seed' 'Grow Verbena' 'Hit Golf Irons'\n",
      " 'Inflate an Air Mattress' 'Install Headphones'\n",
      " 'Install Husky Liners Mud Guards' 'Install a Cooktop'\n",
      " 'Install a Tachometer' 'Insulate a Basement' 'Keep a Busy Life Organized'\n",
      " 'Kill Black Ants' 'Kill Japanese Beetles' 'Make Ackee and Saltfish'\n",
      " 'Make Airbrush Stencils' 'Make Aloe Vera Juice' 'Make Axe Bombs'\n",
      " 'Make Banana Raspberry Pie' 'Make Blueberry Bars' 'Make Bumble Bees'\n",
      " 'Make Carrot Jelly Bean Favors' 'Make Cauliflower Salad'\n",
      " 'Make Chocolate Chip Cookies with Store Bought Dough'\n",
      " 'Make Coconut Pie With Mango Coulis' 'Make Crawfish Etouffe'\n",
      " 'Make Dogs Love You' \"Make Farmer's Cheese\" 'Make Garlic Oil'\n",
      " 'Make Grilled Cinnamon Apples' 'Make Holiday Ice Cream Sandwiches'\n",
      " 'Make Hot and Sour Soup' 'Make Lechon Kawali' 'Make Lollipop Flower Pots'\n",
      " 'Make Orange Julius' 'Make Orange Marmalade' 'Make Perler Bead Keychains'\n",
      " 'Make Pork Chops with Sauteed Apples and Sweet Potatoes'\n",
      " 'Make Red Rice Pudding' 'Make Salt Glaze' 'Make Seafood Fettuccine'\n",
      " 'Make Shiruko' 'Make Strawberry Mousse' 'Make Sugar Cookie Biscuits'\n",
      " 'Make Vegan Artichoke & Spinach Dip' 'Make a Burger King Whopper'\n",
      " 'Make a Cake Vegan' 'Make a Cheesy Gordita Crunch'\n",
      " 'Make a Chesty Cough Drink Remedy' 'Make a Duck Fart'\n",
      " 'Make a Duct Tape Rose Tulip' 'Make a Fake Error Message Using Notepad'\n",
      " 'Make a Gingerbread House' 'Make a Glowstick' 'Make a Good Staff Quickly'\n",
      " 'Make a Grilled Cheese Sandwich on a Griddle'\n",
      " 'Make a Holiday Themed Purse' 'Make a Lime Tart'\n",
      " 'Make a Melon Ball Cocktail'\n",
      " 'Make a New Bar of Soap from Used Bars of Soap' 'Make a Paper Bag Puppet'\n",
      " 'Make a Plantable Seeded Card' 'Make a Quick Quilt' 'Make a Rag Quilt'\n",
      " 'Make a Rainbow Loom Bracelet' 'Make a Soda Can Go \"Pop!\"'\n",
      " 'Make a Solar Hot Air Balloon' 'Make a Sprocket Chandelier'\n",
      " 'Make a Two Ingredient Chocolate Mousse' 'Make an Emergency Pet Harness'\n",
      " 'Make an Origami Whale' 'Paint Upholstery' 'Paint a Fence'\n",
      " 'Peel a Soft Boiled Egg' 'Polish Quartz Crystals' 'Prepare Sooji Halwa'\n",
      " 'Put a Lead Rope on a Horse' 'Remove Glitter from Your Clothes'\n",
      " 'Remove Spark Plugs on a 2010 Dodge Grand Caravan'\n",
      " 'Remove Tar and Asphalt from Clothing' 'Remove a Deer Tick'\n",
      " 'Seal Concrete' 'Set SkyScan Atomic Clock' 'Sew a Perfect Seam'\n",
      " 'Tie a Square Knot' 'Transfer Images to Easter Eggs'\n",
      " 'Use Wicks to Water Plants' 'Use a Cutting Torch'\n",
      " 'View the Character Count for Texts on an iPhone'\n",
      " 'Wall Mount a Plasma TV']\n",
      "\n",
      "Encoded Labels in Training Set: [ 11  97  59  64   6 110 120 104  92  84  71  34 112  91  13   2  47 101\n",
      "  87  25  32   0  79 113  73  83  86  63  35  74 119  27  18  22   3  82\n",
      "  95   4  96  17  55  60 122  23  16 102  42  70  93   1   5   9  48  90\n",
      "  72  10  38  37 105 123  46 118  51  36  68  65 100  88  85  99  61  53\n",
      "  81  19  33  50  76  29 107  12  58  54  43  39  14  98  31  24  52  44\n",
      " 117   7  40 106  78 116 121  45]\n",
      "Encoded Labels in Validation Set: [ 77  26  20 108 115  69 111  57  49  28  66  30  62]\n",
      "Encoded Labels in Testing Set: [109   8  67  56 103  80  41 114  75  21  89  94  15]\n",
      "[features/train/] Missing file: features/train/79VnXfhUJlo.npy\n",
      "[features/train/] Missing file: features/train/V61nB-c5XWw.npy\n",
      "[features/train/] Missing file: features/train/2PapJH_tsiU.npy\n",
      "[features/train/] Missing file: features/train/I9X5I2pJn24.npy\n",
      "[features/train/] Missing file: features/train/FNUs7axgMDk.npy\n",
      "[features/train/] Missing file: features/train/XOYnUblvTrM.npy\n",
      "[features/train/] Missing file: features/train/04MUPw4yays.npy\n",
      "[features/train/] Missing file: features/train/fqVtAiG2SUM.npy\n",
      "[features/train/] Missing file: features/train/qH5LtXUqZSQ.npy\n",
      "[features/train/] Missing file: features/train/yGfrugrO5EE.npy\n",
      "[features/train/] Missing file: features/train/soZMMa3pKyc.npy\n",
      "11 samples were excluded due to missing or invalid .npy files.\n",
      "[features/val/] Missing file: features/val/Y-YZ0Nru37I.npy\n",
      "[features/val/] Missing file: features/val/x8VHmuBUViE.npy\n",
      "[features/val/] Missing file: features/val/AvaZoToCVF4.npy\n",
      "[features/val/] Missing file: features/val/aWlgFMlyvxA.npy\n",
      "4 samples were excluded due to missing or invalid .npy files.\n",
      "[features/test/] Missing file: features/test/bSluSDOICDI.npy\n",
      "[features/test/] Missing file: features/test/ATa9DqRQ8c0.npy\n",
      "2 samples were excluded due to missing or invalid .npy files.\n",
      "Training set size: 87\n",
      "Validation set size: 9\n",
      "Testing set size: 11\n",
      "Batch 0 Feature Shape: torch.Size([16, 1000, 1000])\n",
      "Batch 0 Label Shape: torch.Size([16])\n",
      "Sample Labels: [110 106 119  87  45]\n",
      "Video ID: q-4IL2MadHQ, Feature Shape: (538, 1, 1000)\n",
      "Video ID: bKKvoXMtCls, Feature Shape: (1648, 1, 1000)\n",
      "Video ID: 9lEwYm9mpyE, Feature Shape: (1270, 1, 1000)\n",
      "Video ID: bRtDAsi6UlY, Feature Shape: (494, 1, 1000)\n",
      "Video ID: PvkdCJG4Qc0, Feature Shape: (409, 1, 1000)\n",
      "Video ID: wfHbywyemEc, Feature Shape: (451, 1, 1000)\n",
      "Video ID: co4r-p6JMyQ, Feature Shape: (230, 1, 1000)\n",
      "Video ID: YzvGP69_r6c, Feature Shape: (539, 1, 1000)\n",
      "Video ID: p9eha7EBoVc, Feature Shape: (385, 1, 1000)\n",
      "Video ID: FBfiAX-6ta0, Feature Shape: (250, 1, 1000)\n",
      "Video ID: 7j_jyMoVxWQ, Feature Shape: (229, 1, 1000)\n",
      "Video ID: vdwVLfp467Y, Feature Shape: (166, 1, 1000)\n",
      "Video ID: uHqvgck8evM, Feature Shape: (278, 1, 1000)\n",
      "Video ID: 9PVsNwytzw0, Feature Shape: (401, 1, 1000)\n",
      "Video ID: z8iB0jeGzMM, Feature Shape: (591, 1, 1000)\n",
      "Video ID: OkEnUNGC-Cg, Feature Shape: (46, 1, 1000)\n",
      "Video ID: o1L7vxYYW7o, Feature Shape: (476, 1, 1000)\n",
      "Video ID: RvpvVWzQB50, Feature Shape: (100, 1, 1000)\n",
      "Video ID: W0b63q6Erew, Feature Shape: (452, 1, 1000)\n",
      "Video ID: ly3L70Kmtn4, Feature Shape: (525, 1, 1000)\n",
      "Video ID: Y4mX145-cK0, Feature Shape: (1458, 1, 1000)\n",
      "Video ID: 83fn_DAItBA, Feature Shape: (258, 1, 1000)\n",
      "Video ID: ZThc5fNlMdk, Feature Shape: (577, 1, 1000)\n",
      "Video ID: 2JSmK85Iubg, Feature Shape: (195, 1, 1000)\n",
      "Video ID: MkPAGbVjc0s, Feature Shape: (493, 1, 1000)\n",
      "Video ID: wAKfy-I8WPk, Feature Shape: (339, 1, 1000)\n",
      "Video ID: wubboGsgl_Y, Feature Shape: (638, 1, 1000)\n",
      "Video ID: _DU0jzBSnFo, Feature Shape: (187, 1, 1000)\n",
      "Video ID: TlXX8ft2uDQ, Feature Shape: (358, 1, 1000)\n",
      "Video ID: F-Ik7Xm9lD4, Feature Shape: (220, 1, 1000)\n",
      "Video ID: wr4Wt8Riz68, Feature Shape: (1189, 1, 1000)\n",
      "Video ID: ECcUltP15QQ, Feature Shape: (188, 1, 1000)\n",
      "Video ID: UUEPjA3SG_4, Feature Shape: (160, 1, 1000)\n",
      "Video ID: QSSPetpR4Mk, Feature Shape: (260, 1, 1000)\n",
      "Video ID: EpavkTCk0bQ, Feature Shape: (623, 1, 1000)\n",
      "Video ID: 6yE4_3W0e6M, Feature Shape: (499, 1, 1000)\n",
      "Video ID: Jy2TmhqOeNU, Feature Shape: (459, 1, 1000)\n",
      "Video ID: UBAXnFGwqq4, Feature Shape: (273, 1, 1000)\n",
      "Video ID: kY4hYhMf-Qo, Feature Shape: (233, 1, 1000)\n",
      "Video ID: jG7dSXcfVqE, Feature Shape: (243, 1, 1000)\n",
      "Video ID: IwQ47d1n6cY, Feature Shape: (1011, 1, 1000)\n",
      "Video ID: nOeIsrjlD-c, Feature Shape: (555, 1, 1000)\n",
      "Video ID: 69HF4_z6GIg, Feature Shape: (364, 1, 1000)\n",
      "Video ID: FdiJiG0E1kQ, Feature Shape: (221, 1, 1000)\n",
      "Video ID: VH5dGOJ_yhM, Feature Shape: (211, 1, 1000)\n",
      "Video ID: g8WDiVddh5k, Feature Shape: (69, 1, 1000)\n",
      "Video ID: jGqGDRJ5_FY, Feature Shape: (129, 1, 1000)\n",
      "Video ID: 1eMqBeh2mZc, Feature Shape: (267, 1, 1000)\n",
      "Video ID: g8-tyt7d6jw, Feature Shape: (87, 1, 1000)\n",
      "Video ID: RX9b3rUJGfE, Feature Shape: (169, 1, 1000)\n",
      "Video ID: p_ZWt38d2Vc, Feature Shape: (271, 1, 1000)\n",
      "Video ID: FycvyAbI1jw, Feature Shape: (241, 1, 1000)\n",
      "Video ID: u7iwF-CA5Fo, Feature Shape: (418, 1, 1000)\n",
      "Video ID: GGWgmbj3YQM, Feature Shape: (172, 1, 1000)\n",
      "Video ID: WvP2TzC-ojw, Feature Shape: (138, 1, 1000)\n",
      "Video ID: hmgmeicq51Y, Feature Shape: (232, 1, 1000)\n",
      "Video ID: iPOSk2dOAUo, Feature Shape: (190, 1, 1000)\n",
      "Video ID: S0rZQ8d-z_c, Feature Shape: (97, 1, 1000)\n",
      "Video ID: 0JnStOM2UUE, Feature Shape: (88, 1, 1000)\n",
      "Video ID: 2tM1LFFxeKg, Feature Shape: (271, 1, 1000)\n",
      "Video ID: Hv3NhzWBkWc, Feature Shape: (99, 1, 1000)\n",
      "Video ID: KnK71p5buhk, Feature Shape: (102, 1, 1000)\n",
      "Video ID: yxWzQ0EZ5W4, Feature Shape: (158, 1, 1000)\n",
      "Video ID: QvsWD4Fvm6o, Feature Shape: (238, 1, 1000)\n",
      "Video ID: fRSGpoI7SHE, Feature Shape: (509, 1, 1000)\n",
      "Video ID: NsWchNgxoTs, Feature Shape: (71, 1, 1000)\n",
      "Video ID: pn-6N_7UTuw, Feature Shape: (296, 1, 1000)\n",
      "Video ID: L6r6Xi-nSRk, Feature Shape: (196, 1, 1000)\n",
      "Video ID: 6qWdzg74ciA, Feature Shape: (239, 1, 1000)\n",
      "Video ID: noKQyy0yxcg, Feature Shape: (160, 1, 1000)\n",
      "Video ID: NR8GYeVHc0Q, Feature Shape: (706, 1, 1000)\n",
      "Video ID: 6P9VDV7MfE4, Feature Shape: (77, 1, 1000)\n",
      "Video ID: 1qPVpj9gGpk, Feature Shape: (300, 1, 1000)\n",
      "Video ID: MIXpN4z3_II, Feature Shape: (326, 1, 1000)\n",
      "Video ID: yDe0lIPJz-w, Feature Shape: (419, 1, 1000)\n",
      "Video ID: toOTTE8TNfI, Feature Shape: (675, 1, 1000)\n",
      "Video ID: WEA1XkYkPO4, Feature Shape: (817, 1, 1000)\n",
      "Video ID: ej--Uj59n_E, Feature Shape: (1011, 1, 1000)\n",
      "Video ID: lyd_lOqdT9w, Feature Shape: (144, 1, 1000)\n",
      "Video ID: bKcnjRP63rI, Feature Shape: (315, 1, 1000)\n",
      "Video ID: lhRLtGV6sVE, Feature Shape: (737, 1, 1000)\n",
      "Video ID: gi1TeNIhglM, Feature Shape: (541, 1, 1000)\n",
      "Video ID: at2XbuHG5WQ, Feature Shape: (96, 1, 1000)\n",
      "Video ID: XM3y2JRL980, Feature Shape: (468, 1, 1000)\n",
      "Video ID: KnT8VakRmFU, Feature Shape: (1029, 1, 1000)\n",
      "Video ID: i0ICpjjz8xE, Feature Shape: (475, 1, 1000)\n",
      "Video ID: r2LW0p3oe1o, Feature Shape: (366, 1, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# Combine all unique labels from train, val, and test datasets\n",
    "all_labels = pd.concat([\n",
    "    train_data[\"task_description\"],\n",
    "    val_data[\"task_description\"],\n",
    "    test_data[\"task_description\"]\n",
    "]).unique()\n",
    "\n",
    "# Fit the LabelEncoder on all possible labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "# Debugging: Check label encoding\n",
    "print(\"All Unique Labels Encoded:\", label_encoder.classes_)\n",
    "\n",
    "# Encode labels for each dataset\n",
    "train_data[\"task_description_encoded\"] = label_encoder.transform(train_data[\"task_description\"])\n",
    "val_data[\"task_description_encoded\"] = label_encoder.transform(val_data[\"task_description\"])\n",
    "test_data[\"task_description_encoded\"] = label_encoder.transform(test_data[\"task_description\"])\n",
    "\n",
    "# Debugging: Check encoded labels\n",
    "print(\"\\nEncoded Labels in Training Set:\", train_data[\"task_description_encoded\"].unique())\n",
    "print(\"Encoded Labels in Validation Set:\", val_data[\"task_description_encoded\"].unique())\n",
    "print(\"Encoded Labels in Testing Set:\", test_data[\"task_description_encoded\"].unique())\n",
    "\n",
    "\n",
    "# Dataset class with enhanced handling and proper dimension normalization\n",
    "class VideoFeatureDataset(Dataset):\n",
    "    def __init__(self, feature_dir, label_data, max_length=1000):\n",
    "        self.feature_dir = feature_dir\n",
    "        self.label_data = label_data\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Filter rows with missing or invalid features\n",
    "        initial_count = len(label_data)\n",
    "        self.valid_data = self.label_data[self.label_data[\"video_id\"].apply(self._is_valid_feature)].reset_index(drop=True)\n",
    "        filtered_count = initial_count - len(self.valid_data)\n",
    "\n",
    "        if len(self.valid_data) == 0:\n",
    "            print(f\"All {initial_count} samples were invalid. Please check your .npy files.\")\n",
    "            raise ValueError(\"No valid samples found in the dataset.\")\n",
    "        elif filtered_count > 0:\n",
    "            print(f\"{filtered_count} samples were excluded due to missing or invalid .npy files.\")\n",
    "\n",
    "    def _is_valid_feature(self, video_id):\n",
    "        feature_path = os.path.join(self.feature_dir, f\"{video_id}.npy\")\n",
    "        if not os.path.exists(feature_path):\n",
    "            print(f\"[{self.feature_dir}] Missing file: {feature_path}\")\n",
    "            return False\n",
    "        try:\n",
    "            feature = np.load(feature_path)\n",
    "            # Ensure proper dimensions: (N, 1, 1000) -> (N, 1000)\n",
    "            if feature.ndim == 3 and feature.shape[1] == 1:\n",
    "                feature = feature.squeeze(axis=1)\n",
    "            # Validate final dimensions\n",
    "            if feature.ndim != 2 or feature.shape[1] != 1000:\n",
    "                print(f\"[{self.feature_dir}] Invalid dimensions for {video_id}: {feature.shape}\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"[{self.feature_dir}] Error loading file {video_id}: {e}\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_id = self.valid_data.iloc[idx][\"video_id\"]\n",
    "        label = self.valid_data.iloc[idx][\"task_description_encoded\"]\n",
    "\n",
    "        # Load the feature from .npy file\n",
    "        feature_path = os.path.join(self.feature_dir, f\"{video_id}.npy\")\n",
    "        feature = np.load(feature_path)\n",
    "\n",
    "        # Normalize dimensions if needed\n",
    "        if feature.ndim == 3 and feature.shape[1] == 1:\n",
    "            feature = feature.squeeze(axis=1)  # Convert (N, 1, 1000) -> (N, 1000)\n",
    "\n",
    "        # Pad or truncate the feature to max_length\n",
    "        if feature.shape[0] > self.max_length:\n",
    "            feature = feature[:self.max_length, :]\n",
    "        elif feature.shape[0] < self.max_length:\n",
    "            pad_length = self.max_length - feature.shape[0]\n",
    "            feature = np.pad(feature, ((0, pad_length), (0, 0)), mode='constant')\n",
    "\n",
    "        # Ensure the final feature shape matches (max_length, 1000)\n",
    "        assert feature.shape == (self.max_length, 1000), f\"Feature shape mismatch: {feature.shape}\"\n",
    "\n",
    "        return {\n",
    "            \"feature\": torch.tensor(feature, dtype=torch.float32),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "# Reinitialize datasets and DataLoaders\n",
    "try:\n",
    "    train_dataset = VideoFeatureDataset(\"features/train/\", train_data, max_length=1000)\n",
    "    val_dataset = VideoFeatureDataset(\"features/val/\", val_data, max_length=1000)\n",
    "    test_dataset = VideoFeatureDataset(\"features/test/\", test_data, max_length=1000)\n",
    "\n",
    "    print(f\"Training set size: {len(train_dataset)}\")\n",
    "    print(f\"Validation set size: {len(val_dataset)}\")\n",
    "    print(f\"Testing set size: {len(test_dataset)}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    # Debugging: Inspect a batch from the train DataLoader\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        print(f\"Batch {batch_idx} Feature Shape: {batch['feature'].shape}\")\n",
    "        print(f\"Batch {batch_idx} Label Shape: {batch['label'].shape}\")\n",
    "        print(f\"Sample Labels: {batch['label'][:5].numpy()}\")\n",
    "        break  # Process only the first batch to locate issues\n",
    "\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "# Debugging: Verify all features and their shapes\n",
    "for video_id in train_data[\"video_id\"]:\n",
    "    feature_path = os.path.join(\"features/train\", f\"{video_id}.npy\")\n",
    "    if os.path.exists(feature_path):\n",
    "        feature = np.load(feature_path)\n",
    "        print(f\"Video ID: {video_id}, Feature Shape: {feature.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Fine-Tune Multimodal Model\n",
    "\n",
    "Now that we have extracted and prepared the features, we will:\n",
    "1. Combine the visual, audio, and textual features.\n",
    "2. Fine-tune a multimodal model to predict task descriptions using these features.\n",
    "\n",
    "### Workflow:\n",
    "- Align visual features with textual captions and task descriptions.\n",
    "- Train a transformer-based model or a multimodal architecture like CLIP or VideoBERT.\n",
    "- Evaluate the model's performance on validation and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2571734/2556762115.py:89: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/tmp/ipykernel_2571734/2556762115.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/data1/reu/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 84, in _worker\n    output = module(*input, **kwargs)\n  File \"/data1/reu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/data1/reu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_2571734/2556762115.py\", line 27, in forward\n    combined = torch.cat((visual_out, text_out), dim=1)\nRuntimeError: Tensors must have same number of dimensions: got 3 and 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 121\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# Re-raise unexpected errors\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Attempt to train with dynamic batch adjustment\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m success \u001b[38;5;241m=\u001b[39m \u001b[43msafe_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisual_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m# If out-of-memory, reduce batch size and retry\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     reduced_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "Cell \u001b[0;32mIn[7], line 99\u001b[0m, in \u001b[0;36msafe_train_step\u001b[0;34m(model, visual_features, input_ids, attention_mask, labels, criterion, optimizer, scaler)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Mixed precision training\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[0;32m---> 99\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisual_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Scale loss and backpropagate\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:186\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    185\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 186\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:201\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:109\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    107\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 109\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_utils.py:706\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/data1/reu/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 84, in _worker\n    output = module(*input, **kwargs)\n  File \"/data1/reu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/data1/reu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_2571734/2556762115.py\", line 27, in forward\n    combined = torch.cat((visual_out, text_out), dim=1)\nRuntimeError: Tensors must have same number of dimensions: got 3 and 2\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Define Multimodal Model\n",
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, visual_dim, text_dim, hidden_dim, output_dim):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        # Visual embedding layer\n",
    "        self.visual_fc = nn.Linear(visual_dim, hidden_dim)\n",
    "        # Text embedding layer (using pre-trained BERT)\n",
    "        self.text_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.text_fc = nn.Linear(text_dim, hidden_dim)\n",
    "        # Fusion and output layers\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, visual_features, input_ids, attention_mask):\n",
    "        # Process visual features\n",
    "        visual_out = self.visual_fc(visual_features)\n",
    "        # Process text features\n",
    "        text_out = self.text_model(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "        text_out = self.text_fc(text_out)\n",
    "        # Concatenate and predict\n",
    "        combined = torch.cat((visual_out, text_out), dim=1)\n",
    "        output = self.fc(combined)\n",
    "        return output\n",
    "\n",
    "# Load tokenizer for text processing\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Prepare text data for fine-tuning\n",
    "def tokenize_texts(texts, max_length=128):\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return tokenized['input_ids'], tokenized['attention_mask']\n",
    "\n",
    "# Example batch preparation\n",
    "batch = next(iter(train_loader))  # Fetch a batch from the train DataLoader\n",
    "visual_features = batch['feature']  # Visual features from the dataset\n",
    "labels = batch['label']  # Task description labels\n",
    "\n",
    "# Extract captions for the current batch using label indices\n",
    "batch_labels = batch['label'].tolist()  # Convert tensor to list\n",
    "batch_captions = []\n",
    "\n",
    "for label in batch_labels:\n",
    "    matching_row = train_data[train_data['task_description_encoded'] == label]\n",
    "    if not matching_row.empty:\n",
    "        batch_captions.append(matching_row['processed_captions'].values[0])\n",
    "    else:\n",
    "        batch_captions.append(\"\")  # Fallback if no match is found\n",
    "\n",
    "# Tokenize the text captions for the batch\n",
    "input_ids, attention_mask = tokenize_texts(batch_captions)\n",
    "\n",
    "# Initialize the model\n",
    "visual_dim = 1000  # Dimension of visual features\n",
    "text_dim = 768    # BERT embedding dimension\n",
    "hidden_dim = 512  # Hidden dimension for fusion\n",
    "output_dim = len(label_encoder.classes_)  # Number of task descriptions\n",
    "model = MultimodalModel(visual_dim, text_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Move model to multi-GPU setup if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "# Move batch data to GPU\n",
    "visual_features = visual_features.to(device)\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Mixed precision training setup\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Adjust batch size dynamically if memory errors occur\n",
    "def safe_train_step(model, visual_features, input_ids, attention_mask, labels, criterion, optimizer, scaler):\n",
    "    try:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mixed precision training\n",
    "        with autocast():\n",
    "            outputs = model(visual_features, input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # Scale loss and backpropagate\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Clear unused memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"Training loss for the batch: {loss.item()}\")\n",
    "        return True  # Training successful\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(\"Out of memory error encountered. Reducing batch size or model complexity may help.\")\n",
    "            torch.cuda.empty_cache()\n",
    "            return False  # Training failed\n",
    "        else:\n",
    "            raise  # Re-raise unexpected errors\n",
    "\n",
    "# Attempt to train with dynamic batch adjustment\n",
    "success = safe_train_step(model, visual_features, input_ids, attention_mask, labels, criterion, optimizer, scaler)\n",
    "\n",
    "if not success:\n",
    "    # If out-of-memory, reduce batch size and retry\n",
    "    reduced_batch_size = len(batch['feature']) // 2\n",
    "    if reduced_batch_size > 0:\n",
    "        print(f\"Retrying with reduced batch size: {reduced_batch_size}\")\n",
    "        train_loader = DataLoader(train_dataset, batch_size=reduced_batch_size, shuffle=True)\n",
    "        batch = next(iter(train_loader))  # Fetch new batch with reduced size\n",
    "        visual_features = batch['feature'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        batch_labels = batch['label'].tolist()\n",
    "        batch_captions = [\n",
    "            train_data[train_data['task_description_encoded'] == label]['processed_captions'].values[0]\n",
    "            if not train_data[train_data['task_description_encoded'] == label].empty\n",
    "            else \"\"\n",
    "            for label in batch_labels\n",
    "        ]\n",
    "        input_ids, attention_mask = tokenize_texts(batch_captions)\n",
    "        safe_train_step(model, visual_features, input_ids, attention_mask, labels, criterion, optimizer, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
